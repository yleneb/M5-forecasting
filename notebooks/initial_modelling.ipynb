{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ROOT = pathlib.Path().absolute().parent\n",
    "RAW_DATA_PATH = ROOT / 'data' / 'raw'\n",
    "PROCESSED_DATA_PATH = ROOT / 'data' / 'processed'\n",
    "\n",
    "# endure this project is in the path\n",
    "sys.path.insert(0, ROOT.absolute().as_posix())\n",
    "from src.visualization.visualize import SalesVisualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(PROCESSED_DATA_PATH / 'combined_dataset.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "separating train and valid data\ntrain_df created\nvalid_df created\nseparating target and feature columns\ntrain done\nvalid done\ntransforming features\nx train done\nx valid done\n"
    },
    {
     "data": {
      "text/plain": "<45174237x48 sparse matrix of type '<class 'numpy.float64'>'\n\twith 289648141 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loading dataset')\n",
    "df = pd.read_feather(PROCESSED_DATA_PATH / 'combined_dataset.feather')\n",
    "\n",
    "# just select 1 in 100 products\n",
    "# df = df[df.item_id.str.match(r'.*(1|3|5|7|9)$')]\n",
    "# df = df[df.item_id.str.match(r'.*11$')]\n",
    "\n",
    "# just ause a sample of the dataset for now\n",
    "# sample_df = df.copy()#.sample(frac=0.001, random_state=42)\n",
    "\n",
    "# use the last 28 days for validation\n",
    "print('separating train and valid data')\n",
    "training_end_date = df.date.max()-pd.DateOffset(28)\n",
    "train_df = df[df.date <= training_end_date].copy()\n",
    "print('train_df created')\n",
    "valid_df = df[df.date >  training_end_date].copy()\n",
    "print('valid_df created')\n",
    "\n",
    "del df\n",
    "\n",
    "# separate features and targets\n",
    "print('separating target and feature columns')\n",
    "X_train = train_df.drop('sales', axis=1)\n",
    "y_train = train_df['sales'].copy()\n",
    "print('train done')\n",
    "\n",
    "del train_df\n",
    "\n",
    "X_valid = valid_df.drop('sales', axis=1)\n",
    "y_valid = valid_df['sales'].copy()\n",
    "print('valid done')\n",
    "\n",
    "del valid_df\n",
    "\n",
    "num_attribs = ['sell_price_cent']\n",
    "cat_attribs = ['dept_id','store_id','weekday','month','year']\n",
    "to_drop = [x for x in X_train.columns if x.startswith('event_name')]\n",
    "to_drop += ['item_id','cat_id','state_id','date','event']\n",
    "to_pass = list(set(X_train.columns)-set(num_attribs)-set(cat_attribs)-set(to_drop))\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "    ('drop', 'drop', to_drop)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "print('transforming features')\n",
    "# separate fit and transform so we can parallelize\n",
    "full_pipeline.fit(X_train)\n",
    "\n",
    "X_train_prepared = full_pipeline.transform(X_train)\n",
    "print('x train done')\n",
    "del X_train\n",
    "X_valid_prepared = full_pipeline.transform(X_valid)\n",
    "print('x valid done')\n",
    "del X_valid\n",
    "# del X_train, X_valid\n",
    "\n",
    "X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "train 4.19061487111886\nvalid 3.487933995902688\n\ntrain 1.3290738854407762\nvalid 1.276752570298425\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "lin_reg = LinearRegression(n_jobs=-2)\n",
    "lin_reg.fit(X_train_prepared, y_train)\n",
    "\n",
    "# calculate rmse (not weighted)\n",
    "print('train', mean_squared_error(lin_reg.predict(X_train_prepared), y_train)**0.5)\n",
    "print('valid', mean_squared_error(lin_reg.predict(X_valid_prepared), y_valid)**0.5)\n",
    "print()\n",
    "print('train', mean_absolute_error(lin_reg.predict(X_train_prepared), y_train)**0.5)\n",
    "print('valid', mean_absolute_error(lin_reg.predict(X_valid_prepared), y_valid)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "train 4.261118088869787\nvalid 3.535417111719882\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "rf = ExtraTreesRegressor(n_jobs=-2, max_features=4, bootstrap=True, max_samples=1_000_000, max_depth=4, n_estimators=500)\n",
    "rf.fit(X_train_prepared, y_train)\n",
    "\n",
    "# calculate rmse (not weighted)\n",
    "print('train', mean_squared_error(rf.predict(X_train_prepared), y_train)**0.5)\n",
    "print('valid', mean_squared_error(rf.predict(X_valid_prepared), y_valid)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "train 1.3337578832834487\nvalid 1.2891559268405997\n"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "# calculate rmse (not weighted)\n",
    "print('train', mean_absolute_error(rf.predict(X_train_prepared), y_train)**0.5)\n",
    "print('valid', mean_absolute_error(rf.predict(X_valid_prepared), y_valid)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = np.concatenate(cat_encoder.categories_).tolist()\n",
    "passthrough = list(set(X_train.columns)-set(num_attribs)-set(cat_attribs)-set(to_drop))\n",
    "attributes = num_attribs + cat_one_hot_attribs + passthrough\n",
    "sorted(zip(rf.feature_importances_, attributes), reverse=True)"
   ]
  }
 ]
}